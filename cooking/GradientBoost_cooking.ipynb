{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Performance\n",
    "from time import time\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Helper\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# cross-val\n",
    "from sklearn.cross_validation import cross_val_predict, cross_val_score, train_test_split\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "# nlp\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the training data\n",
    "train = pd.read_json('train.json')\n",
    "# test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>       greek</td>\n",
       "      <td> 10259</td>\n",
       "      <td> [romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> southern_us</td>\n",
       "      <td> 25693</td>\n",
       "      <td> [plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>    filipino</td>\n",
       "      <td> 20130</td>\n",
       "      <td> [eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>      indian</td>\n",
       "      <td> 22213</td>\n",
       "      <td>               [water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>      indian</td>\n",
       "      <td> 13162</td>\n",
       "      <td> [black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class featureExtraction(object):\n",
    "    \"\"\"\n",
    "    featureExtraction represents the stage in which the text data are processed,\n",
    "    or reduced from words/strings to numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"Assign raw data. Assumes dataframe \"\"\"\n",
    "        self.data = data\n",
    "        \n",
    "    def listToString(self):\n",
    "        \"\"\"Extract the unique ingredients. Assumes list of strings and \n",
    "        returns a string.\n",
    "        E.g. input = ['romaine lettuce', 'black olives', 'salt', ...]\n",
    "        E.g. output = 'romaine lettuce black olives salt ...'\n",
    "        \"\"\" \n",
    "        words = [' '.join(item) for item in self.data]\n",
    "        return words\n",
    "    \n",
    "    def stem(self, words):\n",
    "        stemmedTokens = [stemmer.stem(w) for w in words] \n",
    "        return stemmedTokens\n",
    "    \n",
    "    def countVectorize(self, stemmedTokens, max_features=2500, ngram_range=(1,1), \\\n",
    "                  lowercase=True, stop_words=None, max_df=0.5, min_df= 1):\n",
    "        \"\"\"Tokenize and count words.\n",
    "        1. Instantiate vectorizer 'vec'\n",
    "        2. Fit: learn vocabulary and idf from training set\"\"\"\n",
    "        \n",
    "        vec = CountVectorizer(max_features=max_features, ngram_range=ngram_range, \\\n",
    "                              lowercase=lowercase, stop_words=stop_words, max_df=max_df, min_df= min_df)\n",
    "        vec.fit(stemmedTokens)\n",
    "    \n",
    "        return vec\n",
    "    \n",
    "    def tfidfVectorize(self, stemmedTokens, max_features=2500, ngram_range=(1,1), \\\n",
    "                  lowercase=True, stop_words=None, max_df=0.5, min_df= 1):\n",
    "        \"\"\"Tokenize, count, and weight the words.\n",
    "        1. Instantiate vectorizer 'vec'\n",
    "        2. Fit: learn vocabulary and idf from training set\"\"\"\n",
    "        \n",
    "        vec = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range, \\\n",
    "                              lowercase=lowercase, stop_words=stop_words, max_df=max_df, min_df= min_df)\n",
    "        vec.fit(stemmedTokens)\n",
    "        return vec\n",
    "    \n",
    "    def bag_of_words(self, vec, stemmedTokens):\n",
    "        \"\"\"Transform documents to document-term matrix\"\"\"\n",
    "        bag_of_words = vec.transform(stemmedTokens).toarray()\n",
    "        return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39774, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature extraction for training data\n",
    "text = featureExtraction(train.ingredients)\n",
    "trainAsStrings = text.listToString()\n",
    "stemmedStrings = text.stem(trainAsStrings)\n",
    "vec = text.tfidfVectorize(stemmedStrings, max_features=2000, ngram_range=(1,1), \\\n",
    "               lowercase=True, stop_words=None, max_df=0.5, min_df= 1)\n",
    "bow_train = text.bag_of_words(vec, stemmedStrings)\n",
    "bow_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_train, train[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed (s) is: 951.3565318584442\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# fit estimator\n",
    "est = GradientBoostingClassifier(n_estimators=10, max_depth=1, \\\n",
    "                                 max_features = 4, subsample = 0.0005)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTime elapsed (s) is:\", time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.1693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01710979,  0.02005241,  0.02120421,  0.05777035,  0.02171655,\n",
       "        0.06850386,  0.03575906,  0.09357744,  0.020346  ,  0.11397561,\n",
       "        0.02925108,  0.0474852 ,  0.04003642,  0.1165517 ,  0.04607847,\n",
       "        0.01712492,  0.087594  ,  0.03291803,  0.03833207,  0.07461283])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict class labels\n",
    "pred = est.predict(X_test)\n",
    "\n",
    "# score on test data (accuracy)\n",
    "acc = est.score(X_test, y_test)\n",
    "print('ACC: %.4f' % acc)\n",
    "\n",
    "# predict class probabilities\n",
    "est.predict_proba(X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class searchHyperparameters(object):\n",
    "    \"\"\"\n",
    "    Search a model's hyperparameters using either RandomizedSearchCV or\n",
    "    GridSearchCV. Print duration of the grid search and a 'results' method\n",
    "    reveals the best parameters.\n",
    "    \n",
    "    Example usage:\n",
    "        # instantiate algorithm\n",
    "        alg = GradientBoostingClassifier()\n",
    "        # define parameter values to be searched\n",
    "        n_estimators = [1, 3, 5, 10]\n",
    "        max_depth = [2, ]\n",
    "        max_features = [1, 4, 9, 12]\n",
    "        subsample = [0.001, ]\n",
    "        param_grid = dict(n_estimators = n_estimators, max_depth = max_depth, \\\n",
    "                         max_features = max_features, subsample = subsample)\n",
    "        # instantiate searchHyperparameters()\n",
    "        searchHP = searchHyperparameters(alg, param_grid, cval = None, score = None)\n",
    "        # run grid search\n",
    "        grid = searchHP.randomGrid(2, X_train, y_train)\n",
    "        results = searchHP.results(grid)\n",
    "    \"\"\"\n",
    "    def __init__(self, alg, param_grid, cval, score):\n",
    "        \"\"\"Give it an instantiated algorithm \"\"\"\n",
    "        self.alg = alg\n",
    "        self.param_grid = param_grid\n",
    "        self.cval = cval\n",
    "        self.score = score\n",
    "        \n",
    "    def randomGrid(self, n_iter, X, y):\n",
    "        start = time()\n",
    "        grid = RandomizedSearchCV(self.alg, self.param_grid, n_iter = n_iter, cv = self.cval, scoring = self.score)\n",
    "        grid.fit(X,y)\n",
    "        print(\"\\nTime elapsed (s) is:\", time() - start)\n",
    "        return grid\n",
    "    \n",
    "    def fullGrid(self, X, y):\n",
    "        start = time()\n",
    "        grid = GridSearchCV(self.alg, self.param_grid, cv = self.cval, scoring = self.score)\n",
    "        grid.fit(X,y)\n",
    "        print(\"\\nTime elapsed (s) is:\", time() - start)\n",
    "        return grid\n",
    "    \n",
    "    def results(self, grid):\n",
    "        print(\"Overall results:\", grid.grid_scores_)\n",
    "        print(\"Best score:\", grid.best_score_)\n",
    "        print(\"Best parameters:\", grid.best_params_)\n",
    "        print(\"Best model:\", grid.best_estimator_)\n",
    "\n",
    "        # note if SD high, cross-val estimates may not be reliable\n",
    "        results = grid.grid_scores_\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10], 'max_features': [1], 'max_depth': [1], 'subsample': [0.001, 0.004]}\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm\n",
    "alg = GradientBoostingClassifier()\n",
    "# define parameter values to be searched\n",
    "n_estimators = [10]\n",
    "max_depth = [1, ]\n",
    "max_features = [1, ]\n",
    "subsample = [0.001, 0.004]\n",
    "param_grid = dict(n_estimators = n_estimators, max_depth = max_depth, \\\n",
    "                 max_features = max_features, subsample = subsample)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate\n",
    "searchHP = searchHyperparameters(alg, param_grid, None, None)\n",
    "# run grid search\n",
    "grid = searchHP.fullGrid(X_train, y_train)\n",
    "# results = searchHP.results(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall results: [mean: 0.18703, std: 0.00634, params: {'n_estimators': 5, 'max_features': 1, 'max_depth': 1, 'subsample': 0.003}, mean: 0.22142, std: 0.02409, params: {'n_estimators': 10, 'max_features': 1, 'max_depth': 1, 'subsample': 0.001}]\n",
      "Best score: 0.221421387865\n",
      "Best parameters: {'n_estimators': 10, 'max_features': 1, 'max_depth': 1, 'subsample': 0.001}\n",
      "Best model: GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=1, max_features=1, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "              presort='auto', random_state=None, subsample=0.001,\n",
      "              verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "results = searchHP.results(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.18877, std: 0.02000, params: {'n_estimators': 1, 'max_features': 12, 'max_depth': 2, 'subsample': 0.001},\n",
       " mean: 0.16500, std: 0.00425, params: {'n_estimators': 3, 'max_features': 9, 'max_depth': 2, 'subsample': 0.001}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [mean: 0.13973, std: 0.01171, params: {'n_estimators': 3, 'max_features': 12, 'max_depth': 2, 'subsample': 0.001}, \n",
    "#  mean: 0.16500, std: 0.00425, params: {'n_estimators': 3, 'max_features': 9, 'max_depth': 2, 'subsample': 0.001}]\n",
    "[mean: 0.18703, std: 0.00634, params: {'n_estimators': 5, 'max_features': 1, 'max_depth': 1, 'subsample': 0.003},\n",
    " # [mean: 0.18877, std: 0.02000, params: {'n_estimators': 1, 'max_features': 12, 'max_depth': 2, 'subsample': 0.001},\n",
    "    \n",
    "#  mean: 0.19323, std: 0.02066, params: {'n_estimators': 10, 'max_features': 4, 'max_depth': 2, 'subsample': 0.001}]\n",
    "#  mean: 0.20164, std: 0.01122, params: {'n_estimators': 10, 'max_features': 1, 'max_depth': 1, 'subsample': 0.001}]\n",
    "# [mean: 0.21039, std: 0.01131, params: {'n_estimators': 5, 'max_features': 1, 'max_depth': 1, 'subsample': 0.001}, \n",
    " mean: 0.22142, std: 0.02409, params: {'n_estimators': 10, 'max_features': 1, 'max_depth': 1, 'subsample': 0.001}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoostClassifier(object):\n",
    "    \"\"\"\n",
    "    class Boost implements a boost model.\n",
    "    \n",
    "    Set: \n",
    "    number of trees - unlike bagging & random forests, boosting can overfit\n",
    "     if # of trees is too large, although this occurs slowly. Use cross-val to \n",
    "     select # of trees.\n",
    "    learning rate - this controls the rate at which boosting learns. Typical \n",
    "     values are 0.01 and 0.001, and the right choice depends on the problem.\n",
    "     Very small learning rate requires using larger # of trees. \n",
    "    number of splits (d) - often d = 1 works well (a stump). In this case, the\n",
    "     boosted ensemble is fitting an additive model, since each term involves\n",
    "     only a single variable. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train):\n",
    "        \"\"\" \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def gradientBoost(self, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, \\\n",
    "                 min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \\\n",
    "                 max_depth=3, init=None, random_state=None, max_features=None, verbose=0, \\\n",
    "                 max_leaf_nodes=None, warm_start=False, presort='auto'):\n",
    "        \n",
    "        start = time()\n",
    "        # fit estimator\n",
    "        est = GradientBoostingClassifier(loss=loss, learning_rate=learning_rate, n_estimators=n_estimators, \\\n",
    "                                         subsample=subsample, min_samples_split=min_samples_split, \\\n",
    "                                         min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf,\\\n",
    "                                         max_depth=max_depth, init=init, random_state=random_state, max_features=max_features, \\\n",
    "                                         verbose=verbose, max_leaf_nodes=max_leaf_nodes, warm_start=warm_start, presort=presort)\n",
    "        est.fit(X_train, y_train)\n",
    "\n",
    "        print(\"\\nTime elapsed (s) is:\", time() - start)\n",
    "        \n",
    "    def adaBoost(self, base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None):\n",
    "        \n",
    "        start = time()\n",
    "        # fit estimator\n",
    "        est = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=n_estimators, learning_rate=learning_rate, \\\n",
    "                                 algorithm=algorithm, random_state=random_state)\n",
    "        est.fit(X_train, y_train)\n",
    "\n",
    "        print(\"\\nTime elapsed (s) is:\", time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
