{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erin\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Performance\n",
    "from time import time\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Helper\n",
    "import os, sys\n",
    "help_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(help_path)\n",
    "from helper_nlp import featureExtraction\n",
    "from helper_models import searchHyperparameters\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.cross_validation import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the Data\n",
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>       greek</td>\n",
       "      <td> 10259</td>\n",
       "      <td> [romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> southern_us</td>\n",
       "      <td> 25693</td>\n",
       "      <td> [plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>    filipino</td>\n",
       "      <td> 20130</td>\n",
       "      <td> [eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>      indian</td>\n",
       "      <td> 22213</td>\n",
       "      <td>               [water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>      indian</td>\n",
       "      <td> 13162</td>\n",
       "      <td> [black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.197063408257\n"
     ]
    }
   ],
   "source": [
    "# null accuracy (what is accuracy if always predict most prevalent class)\n",
    "# this gives an idea of the underlying distribution of response values\n",
    "train.cuisine.value_counts()\n",
    "train.cuisine.count()\n",
    "null_accuracy = train.cuisine.value_counts().max()/train.cuisine.count()\n",
    "print(null_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see helper_nlp.py for class and methods\n",
    "text = featureExtraction(train.ingredients)\n",
    "trainAsStrings = text.listToString()\n",
    "stemmedStrings = text.stem(trainAsStrings)\n",
    "vec = text.tfidfVectorize(stemmedStrings, max_features=2000, ngram_range=(1,1), \\\n",
    "               lowercase=True, stop_words=None, max_df=0.5, min_df= 1)\n",
    "bow_train = text.bag_of_words(vec, stemmedStrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_train, train[\"cuisine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to-do; fuzzy wuzzy (before feature extraction?); hierarchical selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduce dimension...\n",
    "# from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "# fselect = SelectKBest(chi2 , k=70000)\n",
    "# train_data_features = fselect.fit_transform(bag_of_words_train, train[\"cuisine\"])\n",
    "# test_data_features = fselect.transform(bag_of_words_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed (s) is: 0.6241753339767456\n",
      "Overall results: [mean: 0.72652, std: 0.00781, params: {'alpha': 0.2}, mean: 0.73155, std: 0.00775, params: {'alpha': 0.1}, mean: 0.73134, std: 0.00783, params: {'alpha': 0.05}]\n",
      "Best score: 0.73154542407\n",
      "Best parameters: {'alpha': 0.1}\n",
      "Best model: MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "# instantiate algorithm\n",
    "alg1 = MultinomialNB()\n",
    "# define parameter values to be searched\n",
    "alpha = (0.2, 0.1, 0.05)\n",
    "param_grid = dict(alpha = alpha)\n",
    "# instantiate searchHyperparameters()\n",
    "searchHP_Bayes = searchHyperparameters(alg1, param_grid, cval = 10, score = None)\n",
    "# run grid search\n",
    "grid = searchHP_Bayes.fullGrid(X_train, y_train)\n",
    "results = searchHP_Bayes.results(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time elapsed (s) is: 23.3721724152565\n",
      "Overall results: [mean: 0.77161, std: 0.00264, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 20, 'alpha': 0.0001}, mean: 0.77472, std: 0.00057, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 30, 'alpha': 0.0001}, mean: 0.73553, std: 0.00363, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 20, 'alpha': 1e-05}, mean: 0.73989, std: 0.00376, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 30, 'alpha': 1e-05}]\n",
      "Best score: 0.774723432786\n",
      "Best parameters: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 30, 'alpha': 0.0001}\n",
      "Best model: SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=30, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# instantiate algorithm\n",
    "alg2 = SGDClassifier()\n",
    "# define parameter values to be searched\n",
    "loss = ('modified_huber',)\n",
    "alpha = (0.0001, 0.00001) # constant that multiplies the regularization term\n",
    "penalty = ('elasticnet',) # regularization term\n",
    "n_iter = (20, 30)\n",
    "param_grid = dict(alpha=alpha, loss=loss, penalty=penalty, n_iter=n_iter)\n",
    "# instantiate searchHyperparameters()\n",
    "searchHP_SGD = searchHyperparameters(alg2, param_grid, cval = None, score = None)\n",
    "# run grid search\n",
    "grid = searchHP_SGD.fullGrid(X_train, y_train)\n",
    "results = searchHP_SGD.results(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  mean: 0.57767, std: 0.00147, params: {'penalty': 'elasticnet', 'loss': 'log',          'n_iter': 20, 'alpha': 0.001}, \n",
    "# [mean: 0.65970, std: 0.00221, params: {'penalty': 'elasticnet', 'loss': 'hinge',        'n_iter': 20, 'alpha': 0.001},\n",
    "#  mean: 0.73553, std: 0.00363, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 20, 'alpha': 1e-05}, \n",
    "#  mean: 0.73989, std: 0.00376, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 30, 'alpha': 1e-05}]\n",
    "#   mean: 0.73533, std: 0.00344, params: {'penalty': 'l2',        'loss': 'log',           'n_iter': 10, 'alpha': 0.0001}, \n",
    "#  mean: 0.73909, std: 0.00349, params: {'penalty': 'l2',         'loss': 'log',           'n_iter': 5, 'alpha': 0.0001}, \n",
    "# mean: 0.75612, std: 0.00671, params: {'penalty': 'elasticnet',  'loss': 'modified_huber', 'n_iter': 5, 'alpha': 0.0001}, \n",
    "# mean: 0.76131, std: 0.00244, params: {'penalty': 'elasticnet',  'loss': 'hinge',         'n_iter': 10, 'alpha': 0.0001}]\n",
    "#  mean: 0.76312, std: 0.00341, params: {'penalty': 'elasticnet', 'loss': 'hinge',         'n_iter': 10, 'alpha': 0.0001}]\n",
    "#  mean: 0.76430, std: 0.00056, params: {'penalty': 'elasticnet', 'loss': 'hinge',          'n_iter': 10, 'alpha': 0.0001}]\n",
    "# mean: 0.76470, std: 0.00412, params: {'penalty': 'l2',           'loss': 'modified_huber', 'n_iter': 10, 'alpha': 0.0001}, \n",
    "# [mean: 0.76473, std: 0.00180, params: {'penalty': 'elasticnet', 'loss': 'hinge',           'n_iter': 20, 'alpha': 0.0001},\n",
    "#   mean: 0.76772, std: 0.00341, params: {'penalty': 'l2',         'loss': 'hinge',         'n_iter': 20, 'alpha': 0.0001}, \n",
    "#   mean: 0.76899, std: 0.00229, params: {'penalty': 'l2',        'loss': 'hinge',          'n_iter': 10, 'alpha': 0.0001},\n",
    "#   [mean: 0.77124, std: 0.00315, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 10, 'alpha': 0.0001}, \n",
    "# [mean: 0.77161, std: 0.00264, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 20, 'alpha': 0.0001},\n",
    "#  mean: 0.77342, std: 0.00243, params: {'penalty': 'l2',          'loss': 'modified_huber', 'n_iter': 20, 'alpha': 0.0001}, \n",
    "#   mean: 0.77395, std: 0.00192, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 20, 'alpha': 0.0001}, \n",
    "#  mean: 0.77472, std: 0.00057, params: {'penalty': 'elasticnet', 'loss': 'modified_huber', 'n_iter': 30, 'alpha': 0.0001}, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate algorithm\n",
    "alg3 = RandomForestClassifier()\n",
    "# define parameter values to be searched\n",
    "n_estimators = (10, 50, 100, 150)\n",
    "criterion = ('gini', 'entropy')\n",
    "max_features = (\"sqrt\", \"auto\") # constant that mulitplies the regularization term\n",
    "max_depth = (3, None)\n",
    "param_grid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features, \\\n",
    "                  max_depth=max_depth)\n",
    "# instantiate searchHyperparameters()\n",
    "searchHP_rf = searchHyperparameters(alg3, param_grid, cval = None, score = None)\n",
    "# run grid search\n",
    "grid = searchHP_rf.randomGrid(4, X_train, y_train)\n",
    "results = searchHP_rf.results(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random forest...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training random forest...\")\n",
    "model3 = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "model3.fit(bag_of_words_train, train.cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy:  0.737416402675\n"
     ]
    }
   ],
   "source": [
    "model3_train_pred = cross_val_predict(model3, bag_of_words_train, train.cuisine, cv=2)\n",
    "print(\"Random forest accuracy: \", accuracy_score(train.cuisine, model3_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD accuracy:  0.791934835076\n"
     ]
    }
   ],
   "source": [
    "# with-held sample\n",
    "holdout_pred = grid.predict(X_test)\n",
    "print(\"SGD accuracy: \", accuracy_score(y_test, holdout_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: ['italian' 'cajun_creole' 'mexican' 'thai' 'italian' 'cajun_creole'\n",
      " 'italian' 'chinese' 'mexican' 'southern_us' 'italian' 'mexican' 'mexican'\n",
      " 'indian' 'mexican']\n",
      "True: 30         italian\n",
      "31    cajun_creole\n",
      "32         mexican\n",
      "33            thai\n",
      "34         italian\n",
      "35    cajun_creole\n",
      "36         italian\n",
      "37        filipino\n",
      "38     southern_us\n",
      "39     southern_us\n",
      "40         italian\n",
      "41       brazilian\n",
      "42         mexican\n",
      "43          indian\n",
      "44         mexican\n",
      "Name: cuisine, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# basic comparison (what does model consistently get right or wrong?)\n",
    "# model may make certain types of errors, but not other types of errors\n",
    "# we wouldn't see this behavior merely by looking at accuracy scores\n",
    "print('Pred:', model3_train_pred[30:45])\n",
    "print('True:', train.cuisine[30:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is 0 greek\n",
      "Most frequently predicted cuisine is greek\n",
      "[807  22   1  28   2  20 194  14   4   6   0   0   4   1  45   2   3   1\n",
      "  17   4]\n",
      "Index is 1 southern_us\n",
      "Most frequently predicted cuisine is southern_us\n",
      "[  20 3274   19   30   19   26  216  135   21   47   10    1  214    6  201\n",
      "   11   36    5    7   22]\n",
      "Index is 2 filipino\n",
      "Most frequently predicted cuisine is filipino\n",
      "[  1  40 423   5   2   4  31  26  98   6  19  21   5  15  18  19   8  10\n",
      "   1   3]\n",
      "Index is 3 indian\n",
      "Most frequently predicted cuisine is indian\n",
      "[  22   37   12 2690    9    9   29   57   12    8   27    2    2    4   20\n",
      "   14    7    1   38    3]\n",
      "Index is 4 jamaican\n",
      "Most frequently predicted cuisine is jamaican\n",
      "[  1  44   7  22 373   2  14  13   4   8   4   1   6   4  13   2   5   0\n",
      "   1   2]\n",
      "Index is 5 spanish\n",
      "Most frequently predicted cuisine is spanish\n",
      "[ 18  45  11  10   3 414 187  99   6   7   2   4  24   4 118   4  13   1\n",
      "  12   7]\n",
      "Index is 6 italian\n",
      "Most frequently predicted cuisine is italian\n",
      "[  88  190    8   26    8   52 6856   62   11   20    4    4   29    4  396\n",
      "   12   21    2   21   24]\n",
      "Index is 7 mexican\n",
      "Most frequently predicted cuisine is mexican\n",
      "[  13  149   18   26    7   44  133 5882   20    8    3    0   25    7   70\n",
      "   11    8    5    4    5]\n",
      "Index is 8 chinese\n",
      "Most frequently predicted cuisine is chinese\n",
      "[   2   37   23   21    4    2   44   23 2258    6   41   41   10    2   23\n",
      "   68    0   58    0   10]\n",
      "Index is 9 british\n",
      "Most frequently predicted cuisine is british\n",
      "[  4 140   6  28   7   3  62   6   6 320   2   1   8   5 134   2  51   2\n",
      "   0  17]\n",
      "Index is 10 thai\n",
      "Most frequently predicted cuisine is thai\n",
      "[   2   23   22   75    4    1   15   29  106    2 1124   88    1   12    3\n",
      "   19    0   11    2    0]\n",
      "Index is 11 vietnamese\n",
      "Most frequently predicted cuisine is vietnamese\n",
      "[  2  10  23  12   1   2  18  18  93   1 199 395   1  13  10   9   2  14\n",
      "   0   2]\n",
      "Index is 12 cajun_creole\n",
      "Most frequently predicted cuisine is cajun_creole\n",
      "[   3  183    5    4    5    9   93   36    7   11    0    0 1106    2   67\n",
      "    3    4    1    1    6]\n",
      "Index is 13 brazilian\n",
      "Most frequently predicted cuisine is brazilian\n",
      "[  1  34  17  10   5  16  42  57   3   2  23   5  13 210  22   3   2   0\n",
      "   0   2]\n",
      "Index is 14 french\n",
      "Most frequently predicted cuisine is french\n",
      "[  18  179    5   22    3   39  507   27    9   42    2    3   22    3 1682\n",
      "   10   36    1   10   26]\n",
      "Index is 15 japanese\n",
      "Most frequently predicted cuisine is japanese\n",
      "[  2  25  13 117   2   1  29  13 136   2  16   6   7   3  27 977   6  35\n",
      "   0   6]\n",
      "Index is 16 irish\n",
      "Most frequently predicted cuisine is irish\n",
      "[  9 121   1   9   4  13  46   7   2  47   0   1   4   1  86   3 301   1\n",
      "   5   6]\n",
      "Index is 17 korean\n",
      "Most frequently predicted cuisine is korean\n",
      "[  1  11   8   0   1   3   9   9 128   1   2  17   1   0   4  50   3 575\n",
      "   1   6]\n",
      "Index is 18 moroccan\n",
      "Most frequently predicted cuisine is moroccan\n",
      "[ 23  18   0  67   8  14  47  23   2   3   0   0   2   2  20   2   2   2\n",
      " 583   3]\n",
      "Index is 19 russian\n",
      "Most frequently predicted cuisine is russian\n",
      "[ 12  43   7  15   1   5  54  14   0  24   1   1   5   0  82   5  13   4\n",
      "   4 199]\n"
     ]
    }
   ],
   "source": [
    "labels = train.cuisine.unique()\n",
    "confusion = metrics.confusion_matrix(train.cuisine,model2_train_pred, labels)\n",
    "for ind in range(0,len(labels)):\n",
    "    print(\"Index is\", ind, labels[ind])\n",
    "    print(\"Most frequently predicted cuisine is\", labels[np.argmax(confusion[ind])])\n",
    "    print(confusion[ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = model1.predict_proba( bag_of_words_test )[:,1]\n",
    "p2 = model2.predict_proba( bag_of_words_test )[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step - submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see helper_nlp.py for methods\n",
    "testText = featureExtraction(test.ingredients)\n",
    "testAsStrings = testText.listToString() # transform list to strings   \n",
    "testStemmedStrings = testText.stem(testAsStrings)  # stem the strings\n",
    "bow_test = testText.bag_of_words(vec, testStemmedStrings)  # use the trained vec to transform testStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test set.\n",
    "test_pred = grid.predict(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"id\": test[\"id\"],\n",
    "        \"cuisine\": test_pred\n",
    "    })\n",
    "\n",
    "# Any files you save will be available in the output tab below\n",
    "submission.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
